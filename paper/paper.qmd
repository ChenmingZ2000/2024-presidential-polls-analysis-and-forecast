---
title: "A Hierarchical Bayesian Analysis for Predicting the 2024 US Presidential Election"
subtitle: "Trump's chances of winning are slim"
author: 
  - Chenming Zhao
thanks: "Code and data are available at: [https://github.com/ChenmingZ2000/2024-presidential-polls-analysis-and-forecast.git](https://github.com/ChenmingZ2000/2024-presidential-polls-analysis-and-forecast.git).
        We gratefully acknowledge FiveThirtyEight for providing the polling data used in this analysis. The data is available at: [https://projects.fivethirtyeight.com/polls/president-general/2024/national/](https://projects.fivethirtyeight.com/polls/president-general/2024/national/)."
date: today
date-format: long
abstract: "This study uses a hierarchical Bayesian model to predict whether Donald Trump will win the 2024 U.S. presidential election based on support rates. By analyzing 2024 election data across states, pollsters, and methodologies, we capture the influence of these elements on Trump’s support rate. Our prediction shows Trump has little chance to win due to those key swing states that could affect the outcome. This study also observes the elements shaping public sentiment and provides predictions on the dynamics of support rates in the U.S. election."
format: pdf
number-sections: true
bibliography: references.bib
header-includes:
  - \usepackage{paper/tabularray}
pdf-engine: lualatex
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(dplyr)
library(knitr)
library(ggplot2)
library(kableExtra)
library(bayesplot)
library(patchwork)
library(arrow)
```


# Introduction

The U.S. presidential election has consistently captured the attention of nearly everyone. The 2024 U.S. presidential election is taking place against a backdrop of intense political polarization and complex shifts in public opinion. With changes in the social and economic landscape post-pandemic, the rise of differing ideologies, and frequent political controversies, forecasting the outcome of the 2024 election has become more challenging than ever. Statistics show that the U.S. presidential election is one of the world’s most expensive elections, involving massive amounts of money and manpower, and the 2024 election is expected to cost a record-breaking $15.9 billion [see @nytimes]. Both the Democratic and Republican parties have poured significant resources and funds into the campaign, employing different strategies to win over voters [see @reuters]. The policy stances of different parties directly affect voters and national policies, and these differences will continue to influence the future direction of the electorate, the United States, and even the world. As the world’s largest economy, the U.S. policy shifts directly impact global markets. Changes in U.S. trade policy often lead to shifts in international trade dynamics, affecting the economic development of other countries. Additionally, the U.S. foreign policy and military strategy hold a dominant position in global affairs. The diplomatic stance of the new president may also alter relations with other countries, impacting global security.

In this paper, we focus on the outcome of the 2024 presidential election. Using a dataset from FiveThirtyEight containing extensive polling results, we examine the influence of various factors on candidate support rates by accounting for differences in polling methods, state-level variation, and temporal changes. It is evident that polling methods often fail to fully capture the complexity of public opinion, and factors that introduce bias may intervene from the very design of polling methods. The chaos and complexity of the real world are also reflected in a large-scale political event like the 2024 presidential election, which involves a vast number of participants, wide-ranging groups, and massive investments and interests. Therefore, we employ a hierarchical Bayesian model as a predictive tool in our analysis. This model integrates polling data from FiveThirtyEight covering various states and polling organizations, while also considering the effects of regional and methodological influences. By incorporating variables such as days since Biden’s withdrawal, polling methodology, and whether the poll is national or state-level, the model provides a dynamic view of Trump’s support rate across time and place. With an R² of 0.641 and an RMSE of 2.31, the model effectively captures these patterns while maintaining predictive accuracy, indicating that it can provide meaningful insights into regional and national public opinion trends. Finally, we simulate data for Election Day on November 5 to provide a reliable basis for predicting the election outcome.

Our analysis reveals that differences in Trump’s support rate across regions and polling methods are particularly pronounced. For example, support rates in traditionally conservative states like Texas differ significantly from those in traditionally liberal states like California, while trends in swing states such as Pennsylvania and Michigan are more unstable, with the potential to disrupt the election balance. This underscores the critical impact of geographic and methodological factors on public opinion, aligning well with the historical tendency of swing states to determine the election outcome. Additionally, our results suggest that polling methodologies introduce systematic differences in support rate estimation, with slight variations between results from online panels and phone surveys. In the predictive results, support for Trump at both the state and national levels hovers around 45% to 50%. Comparing this to the support rates of past U.S. presidents, we predict that Trump is unlikely to win in this election.

The remainder of this paper is structured as follows. @sec-data introduces the data sources, preprocessing steps, variable selection criteria, and the potential relationships between independent and dependent variables. @sec-model provides a concise explanation of the hierarchical Bayesian model, including the design rationale and selection of priors and random effects, with more details on the model’s posterior in [Appendix -@sec-model-details]. @sec-results presents the main findings of the study, including a summary of the model output to support its accuracy and predictions of Trump’s support rate across different regions on November 5, 2024. @sec-first in @sec-discussion discusses the implications of the results, @sec-second talks about limitations of the data and polls, complexity of conducting our real world into datas, and future directions.

# Data {#sec-data}

## Overview

Polling data for the 2024 U.S. presidential election is provided by @fivethirtyeight. This dataset compiles information from numerous polls conducted by various pollsters, detailing public support levels for presidential candidates. This data is periodically updated on FiveThirtyEight's website to reflect the latest polling information for the 2024 election. Each record includes information we need like state in which the poll was conducted (or conducted nationally), polling dates (start and end), the specific pollster, the sample size of each poll, the polling methodology, and support percentages for each candidate. This dataset serves as the foundation for analyzing trends and building predictive models for the winner of the upcoming US presidential election. Table tbl-pollingdata below previews this information for polls conducted in 2023 and 2024.

To simulate, test, analyze, and clean the polling data, the statistical programming language R was employed [@citeR]. Key libraries that supported the data analysis include `tidyverse` [@tidyverse], `readxl` [@readxl], `dplyr` [@dplyr], `arrow` [@arrow], `knitr` [@knitr], `ggplot2` [@ggplot2], `kableExtra` [@kableExtra] and `patchwork` [@patch]. Additionally, the `rstanarm` [@rstanarm] library was utilized to implement multilevel modeling for prediction purposes. Also this analysis inherits the folder structure from Professor Rohan Alexander from University of Toronto [@rohan].

## Measurement
	
The process of transforming real-world phenomena into structured data typically involves several critical steps, from designing surveys to final data entry. First, we identify what needs to be measured. In this case, we focus on public opinion regarding the 2024 presidential election, particularly support rates for various candidates. It is essential to define the data scope (e.g., national or state level) and key variables—such as polling information (e.g., polling organization, start and end dates), sample details (e.g., sample size and demographics), and candidate details (e.g., candidate name and party affiliation).

Next, polling organizations develop survey questions based on the desired data, considering sample size, sampling methods, and survey modes to capture respondents' preferences and voting intentions. Different methodologies impact the results, as they reflect the views of distinct groups, which may be influenced by factors like access channels, demographics, and technology usage. For instance, face-to-face surveys tend to capture opinions from offline-inclined groups, whereas voluntary online surveys exclude those less active online.

Once the survey is designed, polling begins, and responses are aggregated by the polling organization. Respondents' answers are recorded and categorized, converting their opinions into structured data points, though these lack of human complexity. To protect privacy, data are often anonymized and aggregated, retaining key insights. After collection, raw data undergo cleaning and processing to minimize sampling biases and errors. Yet, any single poll can have random and systematic error sources, so combining multiple poll results helps mitigate these errors [@blumenthal2010polls].

Processed data are entered into structured datasets. Ideally, well-formatted polling data are stored as individual records, typically including fields like polling organization, sample size, survey dates, margin of error, candidate support percentages, and methodology description. On platforms like FiveThirtyEight, these fields are standardized for ease of comparison and analysis. Finally, FiveThirtyEight and similar platforms aggregate these structured records and present them to the public or allow non-commercial research use, with data usually updated as new polls are released. This enables analysts and the public to track changes in candidate support, analyze trends, and interpret results. An interesting phenomenon, however, is that as election day approaches, poll results often "converge" or "follow" the average, reducing polling error, with evidence suggesting some "final adjustments" in methodology may occur [@blumenthal2010polls].

## Analysis Data
This analysis is focused on Donald Trump, so we only collected observations where the `candidate_name` is Trump. At the same time, we prioritize the quality of the polls. In a political approval rate prediction model, which is often influenced by numerous complex factors, we encounter data that is typically complex and noisy. Low-quality poll results can introduce significant bias into our prediction model, further impacting the accuracy of inferences. Therefore, only polls with a `numeric_grade` above 2.7 were selected. Additionally, Trump's main opponent, Kamala Harris, only entered the race after Biden’s withdrawal, a major political event that could have significant effects on the support rates for both sides. As such, we focused solely on polls conducted after Biden’s withdrawal to enhance the model’s prediction accuracy for the final outcome.

In researching the likelihood of Trump winning, we selected specific variables to focus on those most impactful for predicting approval rates. Here is an overview of data used for this analysis @tbl-view. Following are selected outcome variables and predictor variables, and explainations respectively.

### Outcome Variables
In this analysis of Donald Trump’s support rate in the 2024 U.S. presidential election, the outcome variable is Trump’s support rate (i.e., pct) because it is directly related to the election results. The primary research goal of this analysis is to predict Trump’s chances of winning in the 2024 election, so the support rate is the main metric we aim to model and forecast. The support rate here represents the proportion of respondents in a specific poll who support Trump. We seek to understand how the support rate is influenced by different factors, such as polling methodology, time, or geographical variables. By modeling the fluctuations in Trump’s support rate, we can infer his electoral performance, and assess the impact of various factors on his popularity. Understanding these dynamics provides insights into Trump’s standing relative to his opponents and supports our overall election prediction.

### Predictor Variables
In this analysis, the predictor variables are the factors that we believe can influence the support rate for Trump. These variables are chosen based on their potential to explain changes in support rate. Following are the predictor variables that will be included in our model, along with an explanation of their significance.

**state/national**: Political environments and voter preferences may vary significantly by state, and national polls cannot fully reflect conditions in individual states. The electoral college system in the U.S. makes state-level approval rates important for predicting the likelihood of winning.

**pollster**: Different polling organizations use varying survey methods, sampling strategies, and data-processing procedures, which impact the reliability and accuracy of the poll results. Knowing which polling organization published the data helps assess potential biases in the data.

**methodology**: We anticipate different survey methods significantly impact the representativeness of the results, as they directly influence the characteristics of the sampled population. Some methods may introduce biases toward specific demographic groups.

**is_national**: This variable helps us distinguish between national and state-level polls, ensuring the model can adapt and adjust across different geographic levels.

**days_since_Biden_Withdrawal**: This variable shows the difference between the start day of the poll and the number of days since Biden's withdrawal. Trump's main opponent, Kamala Harris, entered the race only after Biden withdrew. Major political events like Biden’s withdrawal can significantly impact voter approval rates, and tracking the time since this event helps assess both its short-term and long-term effects.

Other potential variables (such as voter demographics like gender, age, race, and income) could indeed influence voter approval rates, but we found that their correlation with presidential election outcomes was not very strong. Additionally, individual voter characteristics are not consistently available in most polls. Thus, we did not include these factors. 

```{r, message=FALSE}
#| echo: false
#| eval: true
#| warning: false
#| message: false

analysis_data <- read_parquet(here::here("data/02-analysis_data/analysis_trump_data.parquet"))
```

```{r}
#| label: tbl-view
#| tbl-cap: The First Few Lines of Cleaned Analysis Data
#| echo: false

kable(head(analysis_data, 10), 
      col.names = c("start_date", "state/national", "pollster", "methodology", "Trump Support Rate", "is_national", "days_since_Biden_Withdrawal"), 
      align = c("l", "c", "c", "c", "c", "c", "c")) %>%
kable_styling(font_size = 7)
```

### Relations Between Variables

The following figures, @fig-pct1 and @fig-pct2, display various aspects of data related to Trump's support rate, helping us understand how support rates vary with different factors. In @fig-pct1, we have marked the occurrence count of each pollster or state on the x-axis, helping us identify potential sources of bias in the data. Low-frequency items (such as data points from certain pollsters or states) may reduce the representativeness of the model—low-frequency observations imply insufficient data for specific pollsters or states, which may not accurately reflect voter support rates in these areas. This incomplete data could lead to unreliable predictions for these pollsters or states, increasing prediction errors. Additionally, low-frequency data is more susceptible to extreme values. With fewer observations, certain outlier data points (e.g., a particularly high or low support rate in a single poll) can appear disproportionately significant, affecting the overall prediction. When fitting the model to low-frequency data, it may be more influenced by the "noise" in these data points, potentially introducing bias into the overall results. Systematic biases in low-frequency observations could spread throughout the model, impacting the overall accuracy of the predictions.

Although low-frequency observations may introduce some bias, directly removing them would result in information loss, especially in election predictions where the representativeness of different states and pollsters is important. Different pollsters and states have their own resources and priorities, leading to uneven data coverage. Some pollsters may prefer to publish data at specific times, while certain states, due to smaller populations or stable voter behavior, may receive less attention. Thus, it is challenging to completely balance the data quantity across different items. Although some observations have low frequency, they still contain valuable information. For instance, even though smaller or remote states may have limited data, their voter preferences are still significant under the Electoral College system. Completely deleting these observations would diminish these states' representation in the overall analysis, weakening the model's accuracy in reflecting support rates. Additionally, we aim to cover as many pollsters and states as possible with the model, so we retain even low-frequency observations. Thus, we keep these observations and mitigate the bias they introduce by combining other variables to build a more robust predictive model.

The plot on the left side in @fig-pct1 shows the average support rates for the top and bottom ten pollsters, highlighting each pollster’s most commonly used methodology. We can observe that the support rates between the top 5 and bottom 5 pollsters differ by about 10%, which helps us understand the bias introduced by certain pollsters’ influence on support rates, as some pollsters may tend to show higher or lower support rates. This chart also examines the different methodologies used by these pollsters, as varying survey methods may affect the results by reaching different groups of voters. Focusing on these differences aids in assessing the reliability and bias of the data.

The plot on the left side in @fig-pct1 compares the average support rates across states with the national average, showing regional differences in voter preferences. Due to the Electoral College system in the U.S. presidential election, variations in state support rates can significantly impact the election outcome. This analysis highlights the differences between state and national polls, helping us understand Trump’s relative support in each state. Paying attention to geographic disparities improves the model's prediction accuracy.

@fig-pct2 displays the trend in support rates over time for the top five pollsters with the most observations. We chose these pollsters because a sufficient number of observations is needed to accurately capture trend changes and reduce the impact of extreme values. Each subplot represents a pollster and shows the change in Trump’s support rate since Biden’s withdrawal. This helps us observe the dynamic change in support rates following major events, and we can also see how different pollsters respond to the same event. Analyzing time trends aids in understanding shifts in public opinion, providing more time-sensitive support rate predictions.

```{r, message=FALSE, fig.width=6, fig.height=3}
#| label: fig-pct1
#| fig-cap: Support rates for Donald Trump which showed in different variables plot 1
#| echo: false

### Group by top 5 and bottom 5 avg pct by pollsters and their most common methodology
# count average pct and occurrence times for each pollster
pollster_summary <- analysis_data %>%
  group_by(pollster, methodology) %>%
  summarize(
    avg_pct = mean(pct, na.rm = TRUE),
    count = n()
  ) %>%
  ungroup()

# find pollsters with top 5 avg pct and bottom 5 avg pct
top5_pollsters <- pollster_summary %>%
  group_by(pollster) %>%
  summarize(
    avg_pct = mean(avg_pct),
    count = sum(count)
  ) %>%
  arrange(desc(avg_pct)) %>%
  slice_head(n = 5) %>%
  mutate(category = "Top 5")

bottom5_pollsters <- pollster_summary %>%
  group_by(pollster) %>%
  summarize(
    avg_pct = mean(avg_pct),
    count = sum(count)
  ) %>%
  arrange(avg_pct) %>%
  slice_head(n = 5) %>%
  mutate(category = "Bottom 5")

# merge
selected_pollsters <- bind_rows(top5_pollsters, bottom5_pollsters) %>%
  select(pollster, category)

# find most commonly used methodology 
selected_pollster_data <- pollster_summary %>%
  filter(pollster %in% selected_pollsters$pollster) %>%
  group_by(pollster) %>%
  filter(count == max(count)) %>%
  ungroup() %>%
  left_join(selected_pollsters, by = "pollster")


p1 <- ggplot(selected_pollster_data, aes(x = reorder(paste(pollster, "(", count, ")", sep = ""), -avg_pct), y = avg_pct, fill = methodology)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ category, scales = "free_x") +
  labs(
    title = "Top 5 and Bottom 5 Avg Support Rate for Donald Trump",
    subtitle = "With Most Common Used Methodologies of Each Pollsters",
    x = "Pollster (Occurrence Count)",
    y = "Average Support Rate (%)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 5, angle = 45, hjust = 1),
    axis.text.y = element_text(size = 5),
    plot.title = element_text(size = 7, face = "bold", color = "#333333"),
    plot.subtitle = element_text(size = 6, color = "#333333"),
    legend.title = element_text(size = 4, color = "#333333"),
    legend.text = element_text(size = 4, color = "#333333"),
    legend.key.size = unit(0.5, "lines"),
    axis.title = element_text(size = 6, color = "#333333"),
    strip.text = element_text(size = 5)
  ) +
  scale_fill_brewer(palette = "Set3") # assign colors to different methodologies


### Group by avg pct of different states and national avg pct
# count average pct and occurrence times for each state category
state_summary <- analysis_data %>%
  group_by(state) %>%
  summarize(
    avg_pct = mean(pct, na.rm = TRUE),
    count = n()
  ) %>%
  ungroup()

# find states with top 5 avg pct and bottom 5 avg pct plus national avg pct
top5_state <- state_summary %>%
  group_by(state) %>%
  summarize(
    avg_pct = mean(avg_pct),
    count = sum(count)
  ) %>%
  arrange(desc(avg_pct)) %>%
  slice_head(n = 5) %>%
  mutate(category = "Top 5")

bottom5_state <- state_summary %>%
  group_by(state) %>%
  summarize(
    avg_pct = mean(avg_pct),
    count = sum(count)
  ) %>%
  arrange(avg_pct) %>%
  slice_head(n = 5) %>%
  mutate(category = "Bottom 5")

national <- state_summary %>%
  group_by(state) %>%
  filter(state == "national") %>%
  summarize(
    avg_pct = mean(avg_pct),
    count = sum(count)
  ) %>%
  arrange(avg_pct) %>%
  mutate(category = "national")

# merge
selected_states <- bind_rows(top5_state, bottom5_state, national) %>%
  select(state, category)

# find most commonly used methodology 
selected_state_data <- state_summary %>%
  filter(state %in% selected_states$state) %>%
  group_by(state) %>%
  ungroup() %>%
  left_join(selected_states, by = "state")


p2 <- ggplot(selected_state_data, aes(x = reorder(paste(state, "(", count, ")", sep = ""), -avg_pct), y = avg_pct, fill = state)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ category, scales = "free_x") +
  labs(
    title = "Average Support Rate for Doanld Trump by states",
    subtitle = "National Average Support Rate Included",
    x = "State Names or National (Occurrence Count)",
    y = "Average Support Rate (%)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 5, angle = 45, hjust = 1),
    axis.text.y = element_text(size = 5),
    plot.title = element_text(size = 7, face = "bold", color = "#333333"),
    plot.subtitle = element_text(size = 6, color = "#333333"),
    legend.title = element_text(size = 5, color = "#333333"),
    legend.text = element_text(size = 5, color = "#333333"),
    legend.key.size = unit(0.5, "lines"),
    axis.title = element_text(size = 6, color = "#333333"),
    strip.text = element_text(size = 5)
  ) +
  scale_fill_brewer(palette = "Set3") # assign colors to different states

p1 + p2
```

```{r, message=FALSE, fig.width= 5, fig.height= 3}
#| label: fig-pct2
#| fig-cap: Support rates for Donald Trump which showed in different variables plot 2
#| echo: false
### Identify the top 10 pollsters by occurrence
top_pollsters2 <- analysis_data %>%
  count(pollster, sort = TRUE) %>%
  top_n(5, n) %>%
  pull(pollster)

data <- analysis_data %>%
  filter(pollster %in% top_pollsters2)

# Assign colors for each pollster
colors <- RColorBrewer::brewer.pal(n = 10, name = "Set3")

### Time trend since Binden's Withdrawal
support_trend_5day <- data %>%
  mutate(grouped_days = (days_since_Biden_Withdrawal %/% 7) * 7) %>% # counted weekly to reduce effect by extreme values
  group_by(grouped_days, pollster) %>%
  summarize(avg_pct = mean(pct, na.rm = TRUE)) %>%
  ungroup()

# Step 2: Create the line plot with background distribution
ggplot(support_trend_5day, aes(x = grouped_days, y = avg_pct)) +
  geom_line(aes(color = pollster), linewidth = 0.7) + # Main line
  geom_jitter(data = data, aes(x = days_since_Biden_Withdrawal, y = pct), 
              color = "#a6cee3", alpha = 0.4, width = 0.5, height = 0.5) +
  scale_x_continuous(
    breaks = seq(min(analysis_data$days_since_Biden_Withdrawal), 
                 max(analysis_data$days_since_Biden_Withdrawal), 
                 by = 14)
  ) +
  labs(
    title = "Average Support Rate for Donald Trump Over Time",
    subtitle = "Days counted weekly since Biden's Withdrawal",
    x = "Days Since Biden's Withdrawal",
    y = "Average Support Rate (%)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 4, angle = 45, hjust = 1),
    axis.text.y = element_text(size = 5),
    plot.title = element_text(size = 7, face = "bold", color = "#333333"),
    plot.subtitle = element_text(size = 6, color = "#333333"),
    legend.title = element_text(size = 5, color = "#333333"),
    legend.text = element_text(size = 4, color = "#333333"),
    legend.key.size = unit(0.5, "lines"),
    axis.title = element_text(size = 6, color = "#333333"),
    strip.text = element_text(size = 5)
  ) +
  facet_wrap(~ pollster, scales = "free_y")
```


# Model {#sec-model}
Our model applies a hierarchical Bayesian approach, based on the framework proposed by @gelman2020regression. This method builds a layered structure that combines polling data collected by different organizations using various methodologies with regional and temporal trends to more accurately predict support rates on Election Day. Running within a Bayesian framework, our model employs a random walk prior to reflect the dynamically changing support rates following Biden's withdrawal in 2024. This prior distribution is based on historical election trends and the latest polling data, allowing the model to continuously update as new data becomes available.

The goal of our modelling strategy is to build a time series model to predict the percentage of support for Trump over time. This model incorporates multiple variables, including the polling methodology, the number of days since Biden's withdrawal from the race on 21 July 2024, whether the poll is at the national level, as well as random effects for each state and each pollster.  These variables are chosen to capture the effects of time, polling characteristics, and geographic location on support rates, allowing us to obtain dynamic estimates of support for Trump and to predict the winner of 2024 U.S. presidential election.

Here we briefly describe the Bayesian analysis model used to investigate the factors influencing support for Trump in the polling data. The model incorporates both fixed effects, such as polling methodology and time since Biden's withdrawal, and random effects to account for variations across states and pollsters. Background details, model assumptions, and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

This model is a generalized linear mixed model (GLMM), specifically:

$y_i \sim \mathcal{N}(\mu_i, \sigma^2)$

Define $y_i$ as the percentage of support (pct) for Trump in poll in the $i$-th observation, assumed to follow a normal distribution with mean $\mu_i$ and variance $\sigma^2$. This model accounts for variations across different methodologies, time since Biden’s withdrawal, and differences between national and state-level polls, with random intercepts for each state and random intercepts and slopes for each pollster.

The linear component of the model is defined as follows: 

$$
\mu_i = \alpha + \beta_1 \cdot \text{methodology}_i+ \beta_2 \cdot \mathrm{days\_since\_Biden\_Withdrawal}_i \\
       + \beta_3 \cdot \mathrm{is\_national}_i + u_{\text{state}[i]} + (v_{\text{pollster}[i]} + \gamma_{\text{pollster}[i]} \cdot \mathrm{days\_since\_Biden\_Withdrawal}_i)
$$

where:

- $y_i$ is the percentage of support (pct) for Trump in poll $i$.
- $\mu_i$ represents the predicted Trump support for poll $i$.
- $\alpha \sim \text{Normal}(45, 5)$ is the global intercept for Trump support.
- $\beta_1 \sim \text{Normal}(0, 5)$ represents the effect of polling methodology on Trump support.
- $\beta_2 \sim \text{Normal}(0, 5)$ represents the effect of days since Biden’s withdrawal.
- $\beta_3 \sim \text{Normal}(0, 5)$ distinguishes national polls from state-level polls.

Random effects:

$u_{\text{state}} \sim \text{Normal}(0, \sigma_{\text{state}})$

$v_{\text{pollster}} \sim \text{Normal}(0, \sigma_{\text{pollster}})$

$\gamma_{\text{pollster}} \sim \text{Normal}(0, \sigma_{\mathrm{pollster\_slope}})$

where:

- $u_{\text{state}[i]}$ is the random intercept for each state, allowing for state-level variations in Trump support.
- $v_{\text{pollster}[i]}$ is the random intercept for each pollster, capturing pollster-specific effects.
- $\gamma_{\text{pollster}[i]}$ is the random slope associated with the time variable ($\mathrm{days\_since\_Biden\_Withdrawal}$) for each pollster.


## Model justification
### Model Prior Selection, Assumptions, and Validation
We based our prior choices on historical polling data and previous research in electoral studies, aiming to provide informative priors without overly constraining the model. For instance, we set a prior on the intercept as $\text{Normal} \sim (45, 5)$, assuming Trump’s support rate centers around 45%, with some uncertainty. Effect size priors (e.g., $\text{Normal} \sim (0, 5)$ for variables such as `methodology`, `days_since_Biden_Withdrawal` and `is_national`) reflect our expectation of moderate effects for these variables. This balance between informative priors and adaptability allows the model to accommodate data while providing a reasonable starting framework.

The model assumes that state-level differences and polling method effects are random and normally distributed. In reality, polling methods and state trends may exhibit non-normal behavior, especially during significant political events. Random effects for different polling agencies are introduced to account for biases associated with varying methodologies. However, we assume that each agency's methodology remains consistent over time, an assumption that may not hold if agencies adjust their sampling techniques. While the model effectively captures overall polling trends, it may not fully reflect subtle shifts within specific demographic subgroups (such as gender, race, socioeconomic status) since these details are not consistently available or may be obscured across poll results.

We implemented the model using the rstanarm package, using MCMC sampling for Bayesian inference. To validate the model, we performed out-of-sample testing and calculated the RMSE (Root Mean Square Error), which resulted in 2.31. In the context of complex, socially influenced political surveys, this error level is considered acceptable. Convergence diagnostics in @fig-post4 in [Appendix -@sec-model-details] show that R-hat values are close to 1, and the Effective Sample Size (ESS) is generally large (often > 400), indicating good independence among samples. Sensitivity analyses on priors indicate that slight adjustments do not significantly alter the predictions, demonstrating model stability.

We also considered a simpler linear regression model which excluding hierarchical structures and a more complex model which incorporating additional economic data like state GDP and personal income. Through testing, we found that the simpler model could not adequately capture differences across polling agencies and states, while the more complex model added computational cost with minimal gains in accuracy. Consequently, we chose the current approach to balance flexibility, interpretability, and computational efficiency.

### Expectation of relationships between variables

We anticipate that different polling `methodology` may affect the estimation of support rates. While we cannot precisely determine which polling method might lean toward which demographic group, it is plausible to assume that methods like "online panels" or "phone surveys" might reach different segments of voters, thus introducing some bias into the support rate estimates. For instance, online surveys may engage more frequent internet users, while phone surveys may cover more traditional media users. This bias could lead to varying estimates of support for Trump.

For the variable `days_since_Biden_Withdrawal`, we hypothesize that, over time, Trump’s approval ratings may fluctuate due to shifts in the election landscape, policy discussions, and changes in voter sentiment. This hypothesis is based on the dynamic nature of polling data: as elections near their end, poll results typically converge, reflecting more accurate forecasts for election day [see @blumenthal2010polls]. We anticipate that the `days_since_Biden_Withdrawal` variable may exhibit a positive or negative trend, depending on the political climate and voter responses to Trump in particular periods.

Through the `is_national` variable, we distinguish between national and state-level polls, this variable enables the model to capture support rate differences arising from geographic and demographic differences. We expect that national polls, which typically have larger and more representative samples, are better suited to capturing overall trends, while state-level polls may reflect finer, localized biases and specific regional issues because national polls often encompass a broader and more balanced voter base, they provide a more stable forecast of overall voter trends. In contrast, due to geographic and demographic variations, state-level polls often exhibit greater fluctuations in error than national polls. These fluctuations stem from regional issues and diverse demographic characteristics, which may not align fully with national trends [see @kennedy2018disentangling].

Therefore, we included a random intercept `(1 | state)` for each state, with the intention that this random intercept would capture the differing impacts each state has on Trump’s support rate. Similarly, by incorporating `(1 + days_since_Biden_Withdrawal | pollster)`, we allow for variability in estimates by different pollsters over time. We anticipate that, due to differences in the stance or methodologies of these organizations, their estimates of Trump’s support rate in the period following Biden’s withdrawal will vary accordingly.


# Results {#sec-results}
## Model Output
Our results are summarized in a table which is available in @tbl-model. This summary reveals the effects of different polling methods and organizations on our estimates. For instance, there is a small difference in estimates between data collected via online panels and phone surveys. The model adjusts for these differences by using a hierarchical structure, which accounts for inconsistencies caused by methodological and organizational biases. Specifically, variance parameters for different methods indicate the different impacts these methods have on sampling coverage and voter behavior, helping the model balance these differences. 

The model’s $R^2$ value is 0.641, indicating that it explains about 64% of the variation in support rates. This shows that the model effectively captures the main patterns in the data, although not every detail. Additionally, the Intraclass Correlation Coefficient (ICC) is 0.9, suggesting a high degree of clustering in the data, meaning that factors like state-specific and polling organization differences significantly contribute to overall variability. Other indicators, such as WAIC (2621.8) and RMSE (2.31), further confirm the model’s good fit, showing that it maintains predictive accuracy without overfitting. Also, to balance precision and flexibility, we set a polling error rate of 3% based on historical data. This helps the model capture typical uncertainty without becoming overly confident. 

```{r, message=FALSE}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

first_model <- readRDS(file = here::here("models/trump_time_model.rds"))

prediction_data <- read_parquet(here::here("data/02-analysis_data/prediction_data.parquet"))
```


```{r}
#| echo: false
#| eval: true
#| label: tbl-model
#| tbl-cap: "TODO"
#| warning: false
#| message: false

modelsummary::modelsummary(
  list("Bayesian model" = first_model),
  statistic = "mad",
  fmt = 2,
  coef_map = c("(Intercept)", "days_since_Biden_Withdrawal", "is_national", 
               "methodologyIVR/Online Panel/Text-to-Web", "Sigma[pollster:(Intercept),(Intercept)]",
               "Sigma[state:(Intercept),(Intercept)]", "Sigma[pollster:days_since_Biden_Withdrawal,days_since_Biden_Withdrawal]",
               "Sigma[pollster:days_since_Biden_Withdrawal,(Intercept)]", "sigma")
)
```

## Prediction Output
From @fig-results, the model predicts that nationwide support for Trump on Election Day, November 5, 2024, will center between 45% to 50% and shows moderate fluctuations. This variability is mainly attributed to national events and differences in polling methods. For instance, nationwide events such as economic shifts or unexpected political occurrences may influence voter sentiment, while differences in polling methods (such as phone surveys versus online panels) may introduce slight fluctuations in estimates due to differences in sample coverage.

At the state level, @fig-results shows predictions indicate significant variations between states. For example, support in Texas is considerably higher than in California, with a difference of over 10 percentage points, while swing states like Pennsylvania and Michigan display greater volatility. This result further validates the model’s robustness; the introduction of a random intercept for each state reflects systematic differences across states. This design aligns with data trends, suggesting that states with diverse geographic and demographic profiles exhibit unique political tendencies.

```{r, fig.width=5, fig.height=5}
#| echo: false
#| eval: true
#| label: fig-results
#| fig-cap: Prediction of average support for Donald Trump on 2024-11-05
#| warning: false
national_only <- prediction_data[prediction_data$state == "national", ]
all_states <- prediction_data[prediction_data$state != "national", ]

p4 <- ggplot(national_only, aes(x = state, y = predicted_support)) +
  geom_point(size = 3, color = "lightblue") +
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2, color = "lightblue") +
  labs(title = "Election Day National Trump Support Rate Prediction", y = "Predictied Support Rate (%)", x = "Area Type in US") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 7),
    axis.text.y = element_text(size = 6),
    plot.title = element_text(size = 8, face = "bold", color = "#333333"),
    axis.title = element_text(size = 7, color = "#333333"))

# Plot for state data
p5 <- ggplot(all_states, aes(x = state, y = predicted_support)) +
  geom_jitter(size = 3, color = "lightblue", width = 0.3) +  # Adds slight horizontal jitter to reduce overlap
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci), width = 0.2, color = "lightblue", alpha = 0.6) +
  geom_smooth(aes(group = 1), method = "lm", color = "orange", se = FALSE) +  # Adds a trend line across all states
  geom_hline(yintercept = 50, color = "black", linetype = "dashed", size = 0.7) +  # Adds a reference line at 50%
  scale_y_continuous(breaks = seq(0, 70, by = 10)) +  # Formats y-axis as percentages with 1 decimal precision
  labs(title = "Election Day Trump Support Rate Prediction by State", y = "Predictied Support Rate (%)", x = "States") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 5, angle = 90, hjust = 1),
    axis.text.y = element_text(size = 6),
    plot.title = element_text(size = 8, face = "bold", color = "#333333"),
    axis.title = element_text(size = 7, color = "#333333"))

p4 + p5 + plot_layout(nrow = 2)
```

# Discussion {#sec-discussion}

## Winner Prediction {#sec-first}
Based on the support rate predictions for Trump at both the national and state levels displayed in @fig-results in @sec-results, it appears challenging for Trump to emerge victorious in the upcoming U.S. presidential election on November 5, 2024, against his primary opponent, Harris. The first chart shows the national support rate prediction for Trump, centering around 45% to 50%. Historically, any candidate needs to reach or exceed the 50% threshold to secure a decisive victory. The current predictions reveal some level of uncertainty, although minor shifts in voter sentiment might occur leading up to Election Day, leaving Trump with some possibility of winning. 

The @fig-results also illustrates the support rates at the state level, revealing significant differences between red and blue states. For instance, Trump’s predicted support rate is higher in traditionally conservative states such as Texas, while in strong Democratic states like California, his support rate is lower. In swing states like Pennsylvania and Michigan, the support rate hovers around 50%, aligning with historical trends where these states tend to have close contests.

To draw final conclusions from these predictions, it is essential to consider historical polling data and past election results. In previous U.S. presidential elections, shifts in sentiment in swing states have played a important role in determining the outcome. For example, the 2016 election highlighted the importance of swing states like Pennsylvania, Michigan, and Wisconsin, where polls underestimated Trump’s support. A study by the Pew Research Center noted that last-minute sentiment shifts and polling errors led to an unexpected advantage for Trump [@pew2021pollinglimitations]. Studies by *The Economist* emphasize the critical impact of swing states on election outcomes, also noting that national polls do not always reflect the dynamics of the Electoral College [see @economist2020electionforecast]. The Electoral College system can result in a candidate winning the popular vote but losing the election, or vice versa. This system has affected U.S. elections multiple times, demonstrating the importance of state-level results relative to national polling trends.

Given the current support rate predictions from our model and the historical performance of swing states, it is projected that Trump cannot secure a clear victory. Furthermore, the volatility in swing states suggests that if these states lean Democratic as they did in 2020, Harris would likely win. However, the final outcome will depend on voter turnout, the last stages of the campaign, and any major events leading up to Election Day.

## Weaknesses and next steps {#sec-second}
In discussing the limitations of polls and our model, we aim to evaluate its robustness and the challenges it faces in capturing the complexities of voter behavior. Each step, from data collection to model construction, involves certain assumptions and potential biases. Below is a detailed discussion of these aspects:

### Defects of Existing Data
Firstly, a major challenge for the model lies in estimating a large number of parameters with limited polling data. This scarcity of data can make the model highly sensitive to choices in priors and interstate covariance structures. For example, relying on historical patterns from states with limited data may fail to reflect the latest shifts in voter sentiment. This issue is also seen across different polling organizations and survey methods. As @gelman2020regression noted, the model can fluctuate between overconfidence and unreasonable uncertainty. To address this, we could consider incorporating alternative data sources, such as social media sentiment or historical population voting trends, to supplement polling data in underrepresented areas. However, these sources carry their own issues, as they may lack reliability and rigor.

Then, in handling systematic bias introduced by different polling methods, the model accounts for variance by assigning specific variance parameters for each method (e.g., phone interviews vs. online panels). Each method may systematically overestimate or underestimate support for a particular candidate. For instance, online panels may attract more politically engaged individuals, leading to biases compared to random phone interviews. To reduce these biases, the model includes variance adjustments specific to each method. Further improvements could involve re-weighting polls based on historical accuracy or introducing dynamic adjustments for newer polling methods. However, as pointed out in @blumenthal2010polls, historical data often include responses to unique events that are not repeatable, making it difficult to assess the accuracy of historical information as a reference. Similarly, the model uses a random walk prior to accommodate fluctuations in public support over time, yet it may struggle to capture abrupt changes driven by political events, such as a debate or controversial statement. The inherent randomness of world events makes it challenging to fully address this problem.

Geographic factors are another focus, with special attention to swing states, which have historically close election results and volatile voter groups. Swing states pose unique challenges for predictive models, as their fluctuations are often inadequately reflected through historical patterns or limited recent polling. Additionally, smaller sample sizes in these states may lead the model to rely too heavily on prior assumptions. Potential solutions include increasing polling frequency in swing states, integrating voter registration data and demographic shifts, or introducing specific polling error adjustments for swing states to enhance model accuracy where small shifts could significantly impact election outcomes.

Furthermore, the polling data we focus on captures voter intent rather than voting probability. Factors such as likelihood of turnout or eligibility are concealed beneath the support rates, and changes in turnout due to major public events may affect election day outcomes. Because these details are not included in the polls, the model cannot fully capture these changes. If the model could dynamically adjust voting behavior estimates based on actual turnout indicators, such as mail ballot requests or early voting numbers, it would better reflect changes in voter behavior and strengthen predictions of the presidential election outcome.

### The Influence of Reality
#### Bias and Transparency
The influence of polling agencies’ biases and methodological transparency on election predictions has long been a research focus. Every polling agency's choices—whether intentional or unintentional—can introduce biases into results. In highly polarized political environments, factors like question wording, sampling techniques, and demographic weighting can amplify potential biases. For example, research from the [Pew Research](https://www.pewresearch.org/short-reads/2024/08/28/key-things-to-know-about-us-election-polling-in-2024/) Center and [Stanford University](https://news.stanford.edu/stories/2020/10/9-things-know-election-polling-data) shows that differences in methods, such as educational weighting or nonresponse adjustments, can lead to significant polling biases, especially for politically sensitive figures like Donald Trump [see @pew2024electionpolling and @stanford2020electionpolling].

Another complex issue is "partisan nonresponse bias," where certain political groups are less likely to participate in polls. This trend has become more prominent in recent years, notably affecting conservative respondents. Pollsters must take extra care in sampling to accurately represent all population segments, which presents particular challenges in practice and makes it hard to eliminate these biases entirely [see @pew2024electionpolling].

Additionally, there is a feedback loop between published poll results and voter behavior: trends shown in polls can influence undecided voters or shift the enthusiasm levels of a candidate's supporters. The presence of "shy" or undecided voters further complicates the challenge of accurate polling. Despite methodological advancements, polling remains prone to errors that are often underestimated compared to the actual unpredictability of elections.

#### Financial Constraints
Financial limitations are a major barrier to achieving ideal polling coverage, particularly when trying to gather representative polling data across all U.S. states. Polling each state comprehensively is costly, especially in low-population states or states deemed "safe" for one candidate. In these cases, data scarcity means the model largely relies on data from more populous or swing states, which may not fully represent the diversity of the nationwide electorate, introducing potential biases.

According to a [Pew Research Center report](https://www.pewresearch.org/methods/2021/04/08/confronting-2016-and-2020-polling-limitations/), polling biases often stem from financial constraints and methodological choices, such as the use of cost-effective online panels, which may not effectively capture perspectives from older, rural, or technologically less-adept groups. These groups often require different recruitment strategies and data collection methods, increasing polling costs [see @pew2021pollinglimitations]. Another [Pew report](https://www.pewresearch.org/politics/2024/09/09/issues-and-the-2024-election/) highlights that participation in extreme partisan regions is often low, which makes balanced representation more challenging and increases the financial and logistical burden of implementing corrective weighting and sampling techniques [see @pew2024issues].

In theory, expanding polling coverage could improve model representativeness and accuracy. However, due to the high cost of polling and the statistical law of diminishing marginal returns, this expansion is often constrained by budget and logistical support. The concept of diminishing marginal returns means that as more resources (like time, money, or personnel) are invested in an activity, the additional benefit from each extra unit of investment gradually decreases. For instance, the first slice of pizza might be very satisfying, but by the third or fourth slice, enjoyment decreases. For polling, investing resources to cover more regions initially increases data representativeness significantly, but as major groups are covered, further investments yield smaller improvements, inevitably leaving certain groups underrepresented in the results.

#### The Balance Between Model Complexity and Interpretability
While our model integrates multiple parameters and hierarchical structures to address the variability across states and polling methodologies for a more accurate election support forecast, this complexity also introduces significant challenges in interpretability. For non-expert users, understanding how random intercepts or variance parameters influence the final prediction requires a certain level of statistical knowledge. Even though we attempt to make the model’s main drivers (e.g., state-level effects or specific methodological impacts) more understandable through simplified summaries and visualizations, several issues remain.

Visualizations often struggle to fully capture the complex structure of our model. The model includes data from 50 U.S. states, multiple polling organizations, and various survey methodologies. If we attempted to display every detail within a chart, the visualizations would become overwhelmingly complex, making it difficult for readers to extract key insights. Conversely, if we limit the display to a subset, such as only highlighting key states or selected polling methods, this approach risks losing the completeness of information. These simplified visualizations fail to convey all sources of variation within the model, potentially leading readers to overlook significant factors and misunderstand the impact of omitted influences. Additionally, this partial representation can foster a feedback loop where oversimplified summaries influence potential voters' perceptions, impacting the trends in unpredictable ways.

#### The Randomness of the Real World
Voter preferences can shift due to events like debates, economic changes, or international incidents, challenging election predictions. Regular polling may fail to capture these shifts if not updated frequently, leading to outdated and potentially misleading forecasts. The [Harvard Data Science Review](https://hdsr.mitpress.mit.edu/pub/nw1dzd02/release/2) highlighted this uncertainty in the 2020 election, where state-level opinion volatility was influenced by economic and educational factors, necessitating models that incorporate these dynamic interactions [@gelman2020regression].

With elections occurring only every four years, opportunities to validate prediction models are limited. Standard techniques like cross-validation are challenging to implement due to data scarcity, pushing models to rely heavily on historical data, which risks overfitting to past trends. Harvard Data Science Review notes that this reliance can result in models exhibiting excessive certainty or unreasonable uncertainty, especially when recent polling data is lacking.

\newpage

\appendix

# Appendix {-}





# Additional data details

# Model details {#sec-model-details}
R package `bayesplot` [@bayesplot] is used for posteriors.

Due to space limitations, we have chosen to display only the posterior and prior comparison for one polling methodology variable in the posterior analysis. If we were to display all methodology variables, the chart would become overly complex and difficult to interpret. Although the effects of multiple methodology variables differ, displaying only one variable may make the model’s diversity and sensitivity to biases across different polling methods appear insufficient. Here, we have selected a representative methodology, which allows us to avoid being distracted by excessive data. The output results for other methodologies are similar to IVR/Online Panel/Text-to-Web or hover around zero due to limited data.

## Posterior predictive check
@fig-post1 presents a comparative display of the prior and posterior distributions for selected parameters in our model. These parameters include the model’s intercept (Intercept), a polling method-related variable (“mIVR/OP/T--W.” representing `IVR/Online Panel/Text-to-Web`), a flag indicating whether the poll is national (is_national), the number of days since Biden’s withdrawal (dys_snc_Bd_W.), and the model's error term (sigma). The prior distributions represent our initial expectations for these parameters before incorporating the data, while the posterior distributions reflect the influence of actual data on these estimates. By comparing the priors and posteriors, we can observe how the data has refined the estimates for each parameter. For instance, the prior for the IVR/Online Panel/Text-to-Web variable is relatively broad, indicating initial uncertainty about its impact, whereas the posterior distribution is much narrower, suggesting that the data supports a certain level of effect from this polling method.

The intercept here represents the baseline estimate of Trump’s support rate when all other variables are zero. The posterior distribution for the intercept is centered around 50, suggesting that the model’s baseline estimate for Trump’s support rate is approximately 50%, which is relatively high. Additionally, the posterior distributions for `is_national` and `days_since_Biden_Withdrawal` are close to zero, indicating that these variables have minimal or uncertain impact on the support rate. The error term (sigma) represents the model's estimation of uncertainty in the data. A higher posterior mean for sigma suggests that the model detects a moderate level of variability in the data, possibly due to fluctuations in public opinion across states or over time.

We observe that the posterior distribution for `is_national` is concentrated near zero, indicating that there is no significant difference in support rates between national and state-level polls. Similarly, `dys_snc_Bd_W.` is a variable capturing the time elapsed since Biden’s withdrawal, intended to reflect any time-related influence. The posterior distribution near zero suggests that this variable has no substantial effect on Trump’s support rate, possibly indicating that public opinion on Trump has remained relatively stable over time. The `mIVR/OP/T--W.` parameter represents a specific polling method (“IVR/Online Panel/Text-to-Web”), which is included to capture the impact of different polling methods on support rates. The posterior indicates that while the influence of this method is relatively limited, it still contributes some variability to the model.
```{r, fig.width=5, fig.height=3}
#| echo: false
#| eval: true
#| label: fig-post1
#| fig-cap: Comparison of Prior and Posterior Distributions for Selected Model Parameters
#| warning: false

posterior_vs_prior(first_model, pars = c("(Intercept)", "methodologyIVR/Online Panel/Text-to-Web", "days_since_Biden_Withdrawal", "sigma", "is_national")) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```
@fig-post2 shows a comparison between the distribution of observed data $y$ and the distribution of simulated data $y_{rep}$ generated by our model. The dark solid line represents the distribution of the actual observed data, while the multiple light lines represent the distribution of multiple simulated datasets generated by the model. This comparison helps assess the model's fit quality that if the distribution of simulated data $y_{rep}$ closely matches that of the observed data $y$ it then suggests that the model effectively captures the characteristics of the observed data, demonstrating good predictive capability and a reasonable posterior distribution.

In @fig-post2, we see that the distribution of the simulated data (light lines) aligns closely with the real data distribution (dark line), especially in terms of the peak and shape of the distribution. This indicates that the data generated by our model is consistent with the actual data, suggesting that the model can adequately replicate the distributional characteristics of the real data. This comparison validates the model's predictive ability and reliability, giving us greater confidence in the model's subsequent predictions.

```{r, fig.width=5, fig.height=3}
#| echo: false
#| eval: true
#| label: fig-post2
#| fig-cap: PP Check for Model
#| warning: false


pp_check(first_model)
```


@fig-post3 shows the MCMC sampling trace plots for four key parameters in our model, including the `IVR/Online Panel/Text-to-Web` method variable, the `days_since_Biden_Withdrawal` variable, the error term `sigma`, and the intercept term. From Chain 1 to Chain 4, with each chain representing an independent MCMC sampling process. Our expectation is that each sampling chain will stabilize and fluctuate around a central value without showing a clear trend (i.e., the mean of the samples converges over time), and there will be no significant differences between the chains. If these conditions are met, it indicates that the model has reached convergence, and the posterior distribution is reliable. From @fig-post3, we observe that each parameter’s sampling chains display a certain level of stability, with the chains fluctuating around a central value after reaching a steady state, without showing upward or downward trends. The distributions across chains are similar, indicating consistency in sampling results and supporting model convergence. This suggests that our model has successfully achieved a stable state within the posterior distribution, and the sampling results are reliable.

```{r, fig.width=5, fig.height=3}
#| echo: false
#| eval: true
#| label: fig-post3
#| fig-cap: MCMC sampling trace plots for model
#| warning: false


plot(first_model, "trace", pars = c("methodologyIVR/Online Panel/Text-to-Web", "days_since_Biden_Withdrawal", "sigma", "(Intercept)"))
```
This plot shows the $\hat{R}$ diagnostic statistic for our model, which is commonly used to assess the convergence of MCMC sampling. Ideally, the $\hat{R}$ value should be close to 1, indicating consistent variance across different MCMC chains, signifying that the model has reached a stable state. The horizontal axis represents the $\hat{R}$ values, where values closer to 1 indicate better convergence. Our expectation is that all parameters have $\hat{R}$ values below 1.05, indicating minimal variance differences between chains and consistency across them. This plot shows that nearly all $\hat{R}$ values are concentrated around 1, with all falling below the threshold of 1.05 (indicated by the vertical dashed line on the right), suggesting that the sampling chains have converged well, providing high reliability for the model results.

```{r, fig.width=5, fig.height=3}
#| echo: false
#| eval: true
#| label: fig-post4
#| fig-cap: R hat Diagnostic statistic for model
#| warning: false

plot(first_model, "rhat")
```

@fig-post5 shows the posterior distributions of certain parameters in our model, including the `(Intercept)`, a methodology variable `methodologyIVR/Online Panel/Text-to-Web`, a time variable `days_since_Biden_Withdrawal`, and the error term `sigma`. From the shape and location of the posterior distributions, we can see the degree to which the observed data supports the estimates of these parameters and the range of uncertainty around them. The `(Intercept)` distribution is centered around 30% to 55%, suggesting that the model estimates a 45% baseline support rate, which aligns with our expectations for Trump’s support rate. The methodology variable shows a relatively wide distribution, indicating that the data does not provide strong directional support for its influence, suggesting that it may have a minimal or uncertain impact on the support rate. Also, the time variable has a posterior distribution close to zero, indicating that the number of days since Biden’s withdrawal does not significantly impact the support rate. The error term `sigma` is at a low level, suggesting that the model views the data as having low volatility and being relatively concentrated. Thus, we can conclude that the model effectively captures the significant impact of the baseline support rate but but due to data uncertainty, it struggles to reveal the influence of factors like methodology and time. This phenomenon may reflect inconsistencies in the data across different poll methods or limitations in the the sample.

```{r, fig.width=5, fig.height=3}
#| echo: false
#| eval: true
#| label: fig-post5
#| fig-cap: posterior distributions of four parameters in model
#| warning: false

plot(first_model, "areas", pars = c("methodologyIVR/Online Panel/Text-to-Web", "days_since_Biden_Withdrawal", "sigma", "(Intercept)"))
```

## Diagnostics


\newpage


# References

